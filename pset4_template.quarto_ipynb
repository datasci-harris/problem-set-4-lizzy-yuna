{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"PS4\"\n",
        "format: \n",
        "  pdf:\n",
        "    keep-tex: true\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "---\n",
        "\n",
        "\n",
        "**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. \n",
        "We use (`*`) to indicate a problem that we think might be time consuming. \n",
        "\n",
        "## Style Points (10 pts) \n",
        "Please refer to the minilesson on code style\n",
        "**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.\n",
        "\n",
        "## Submission Steps (10 pts)\n",
        "1. This problem set is a paired problem set.\n",
        "2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.\n",
        "    - Partner 1 (name and cnet ID): Yuna Baek / ybaek / 12370259\n",
        "    - Partner 2 (name and cnet ID): Lizzy Diaz / lizzydiaz / endiaz\n",
        "\n",
        "3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. \n",
        "4. \"This submission is our work alone and complies with the 30538 integrity policy.\" Add your initials to indicate your agreement: \\*\\*\\*YB\\*\\*, \\*\\*\\*LD\\*\\*\n",
        "5. \"I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**\"  (1 point)\n",
        "6. Late coins used this pset: \\*\\*\\_\\_\\*\\* Late coins left after submission: \\*\\*\\_3_\\*\\* (Yuna), \\*\\*\\_2_\\*\\* (Lizzy)\n",
        "7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, \n",
        "    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. \n",
        "8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.\n",
        "9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.\n",
        "10. (Partner 1): tag your submission in Gradescope\n",
        "\n",
        "\n",
        "## Download and explore the Provider of Services (POS) file (10 pts)"
      ],
      "id": "6e4f4e43"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "alt.renderers.enable(\"png\")\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings \n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "9b8bb75d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Read through the rest of the problem set and look through the data dictionary to identify which variables you will need to complete the exercise, and use the tool on data.cms.gov into restrict to those variables (“Manage Columns”) before exporting (“Export”). Download this for 2016 and call it pos2016.csv. What are the variables you pulled?\n",
        "\n",
        "PRVDR_CTGRY_CD, PRVDR_CTGRY_SBTYP_CD, PRVDR_NUM, PGM_TRMNTN_CD, FAC_NAME, ZIP_CD\n",
        "\n",
        "PRVDR_CTGRY_CD: Identifies the type of provider participating in the Medicare/Medicaid program. 01=Hospital\n",
        "\n",
        "PRVDR_CTGRY_SBTYP_CD: Identifies the subtype of the provider, within the primary category. Used in reporting to show the breakdown of provider categories, mainly for hospital and SNFs.\n",
        "01 = short term\n",
        "\n",
        "PRVDR_NUM: Six or ten position identification number that is assigned to a certified provider. This is the CMS Certification Number.\n",
        "\n",
        "PGM_TRMNTN_CD: Indicates the current termination status for the provider.\n",
        "00=ACTIVE PROVIDER\n",
        "01=VOLUNTARY-MERGER, CLOSURE\n",
        "02=VOLUNTARY-DISSATISFACTION WITH REIMBURSEMENT\n",
        "03=VOLUNTARY-RISK OF INVOLUNTARY TERMINATION\n",
        "04=VOLUNTARY-OTHER REASON FOR WITHDRAWAL\n",
        "05=INVOLUNTARY-FAILURE TO MEET HEALTH/SAFETY REQ\n",
        "06=INVOLUNTARY-FAILURE TO MEET AGREEMENT\n",
        "07=OTHER-PROVIDER STATUS CHANGE\n",
        "08=NONPAYMENT OF FEES - CLIA Only\n",
        "09=REV/UNSUCCESSFUL PARTICIPATION IN PT - CLIA Only\n",
        "10=REV/OTHER REASON - CLIA Only\n",
        "11=INCOMPLETE CLIA APPLICATION INFORMATION - CLIA Only \n",
        "\n",
        "FAC_NAME: Name of the provider certified to participate in the Medicare and/or Medicaid programs.\n",
        "\n",
        "ZIP_CD:  Five-digit ZIP code for a provider's physical address.\n",
        "\n",
        "2. We want to focus on short-term hospitals. These are identified as facilities with provider type code 01 and subtype code 01. Subset your data to these facilities. How many hospitals are reported in this data? Does this number make sense? Cross-reference with other sources and cite the number you compared it to. If it differs, why do you think it could differ?\n"
      ],
      "id": "6fc08ef3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Change path variable to read in files from local \n",
        "# path = r\"C:\\Users\\User\\OneDrive - The University of Chicago\\4_DAP-2\\PS4 Data\"\n",
        "path = \"/Users/yunabaek/Desktop/3. Python II/problem-set-4-lizzy-yuna/\"\n",
        "\n",
        "def load_dataframes(path): \n",
        "    pos_16 = pd.read_csv(os.path.join(path, 'pos2016.csv'), encoding='ISO-8859-1') \n",
        "    pos_17 = pd.read_csv(os.path.join(path, 'pos2017.csv'), encoding='ISO-8859-1') \n",
        "    pos_18 = pd.read_csv(os.path.join(path, 'pos2018.csv'), encoding='ISO-8859-1') \n",
        "    pos_19 = pd.read_csv(os.path.join(path, 'pos2019.csv'), encoding='ISO-8859-1') \n",
        "    return pos_16, pos_17, pos_18, pos_19\n",
        "\n",
        "pos_16, pos_17, pos_18, pos_19 = load_dataframes(path)\n",
        "\n",
        "# This is initially how we read answered question 1. We changed it to the above code to make switching between partners much easier. \n",
        "# path = \"/Users/yunabaek/Desktop/3. Python II/problem-set-4-lizzy-yuna/\"\n",
        "# pos_16 = pd.read_csv(\"pos2016.csv\")\n",
        "pos_16.info()"
      ],
      "id": "b71a11db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pos_16 = pos_16[(pos_16['PRVDR_CTGRY_SBTYP_CD'] == 1) & (pos_16['PRVDR_CTGRY_CD'] == 1)]\n",
        "\n",
        "unique_fac_names = len(pos_16['FAC_NAME'].unique())\n",
        "number_rows = len(pos_16)\n",
        "print(f\"There are {number_rows} rows and {unique_fac_names} unique hospitals.\")"
      ],
      "id": "a33709c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    a. There are 6,770 hospitals reported in the data when subsetted by hospital name. This seems like an overestimate, which could be because the data have redundancies (the same hospital under different names, for example). \n",
        "\n",
        "    b. The issue brief from KFF states that there are nearly 5000 short-term hospitals acute care hospitals in the United States (Wishner et al., 2016). This difference may be due to the different definitions in short-term hospitals. It could also be due to differences in when the data were reported. \n",
        "\n",
        "3. Repeat the previous 3 steps with 2017Q4, 2018Q4, and 2019Q4 and then append them together. Plot the number of observations in your dataset by year.\n",
        "\n",
        "Loading 17-19 data (Note: This is the code we originally wrote. Changed in favor of code written above in 2a).\n",
        "\n",
        "```{bash}\n",
        "# pos_17 = pd.read_csv(\"pos2017.csv\")\n",
        "# encountered error for 2018, 2019: 'utf-8' codec can't decode byte 0x98 in position 11050: invalid start byte\n",
        "# pos_18 = pd.read_csv(\"pos2018.csv\", encoding='ISO-8859-1')\n",
        "# pos_19 = pd.read_csv(\"pos2019.csv\", encoding='ISO-8859-1')\n",
        "```\n",
        "\n",
        "\n",
        "Subsetting data"
      ],
      "id": "6ff88979"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pos_17 = pos_17[(pos_17['PRVDR_CTGRY_SBTYP_CD'] == 1) & (pos_17['PRVDR_CTGRY_CD'] == 1)]\n",
        "\n",
        "pos_18 = pos_18[(pos_18['PRVDR_CTGRY_SBTYP_CD'] == 1) & (pos_18['PRVDR_CTGRY_CD'] == 1)]\n",
        "\n",
        "pos_19 = pos_19[(pos_19['PRVDR_CTGRY_SBTYP_CD'] == 1) & (pos_19['PRVDR_CTGRY_CD'] == 1)]"
      ],
      "id": "f9aa1677",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Appending data"
      ],
      "id": "ea2f1350"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pos_16['Year'] = 2016\n",
        "pos_17['Year'] = 2017\n",
        "pos_18['Year'] = 2018\n",
        "pos_19['Year'] = 2019\n",
        "pos_df = pd.concat([pos_16, pos_17, pos_18, pos_19])\n",
        "pos_df.info()"
      ],
      "id": "7eb6dead",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "obs_df = pos_df['Year'].value_counts().reset_index()\n",
        "obs_df.columns = ['Year', 'Observations']\n",
        "\n",
        "obs_plot = alt.Chart(obs_df).mark_bar().encode(\n",
        "    x=alt.X('Year:O', title='Year',\n",
        "    axis=alt.Axis(labelAngle=0)),\n",
        "    y=alt.Y('Observations:Q', title = 'Number of Observations',\n",
        "    scale=alt.Scale(domain=[6000, 7500])),\n",
        ").properties(\n",
        "    title= 'Number of Observations by Year',\n",
        "    width=550,\n",
        "    height=300\n",
        ").configure_scale(\n",
        "    clamp=True\n",
        ")\n",
        "\n",
        "obs_plot"
      ],
      "id": "bbb77d69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Each hospital is identified by its CMS certification number. \n",
        "\n",
        "    a. Plot the number of unique hospitals in your dataset per year. "
      ],
      "id": "e54b3803"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cms_df = pos_df.groupby('Year')['PRVDR_NUM'].nunique().reset_index()\n",
        "cms_df.columns = ['Year', 'Unique_CMS']\n",
        "\n",
        "cms_plot = alt.Chart(cms_df).mark_bar().encode(\n",
        "    x=alt.X('Year:O', title='Year',\n",
        "    axis=alt.Axis(labelAngle=0)),\n",
        "    y=alt.Y('Unique_CMS:Q', title = 'Number of Unique Hospitals',\n",
        "        scale=alt.Scale(domain=[6000, 7500]))\n",
        ").properties(\n",
        "    title= 'Number of Unique Hospitals by Year',\n",
        "    width=550,\n",
        "    height=300\n",
        ").configure_scale(\n",
        "    clamp=True\n",
        ")\n",
        "cms_plot"
      ],
      "id": "af1f35f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    b. Compare this to your plot in the previous step. What does this tell you about the structure of the data?\n",
        "\n",
        "The two plots are almost identical, suggesting that the data structure is consistent. There is a stable number of unique records per hospital over the years, indicating that the dataset effectively captures hospital activities without significant duplication. This implies that the dataset is likely well-maintained. The slight upward trend across both plots may reflect real growth in the number of hospitals or an increase in healthcare activity over the years. It may be useful to explore external factors influencing this trend, such as policy changes or demographic shifts.\n",
        "\n",
        "## Identify hospital closures in POS file (15 pts) (*)\n",
        "\n",
        "1. In this question, I am treating each provider number as a unique hospital. This is because I ran into an issue where some hospitals, for each year, had both a 0 and 1 termination code, and different provider numbers. Using the provided definition, there were 174 hospitals open in 2016 but closed by 2019. \n",
        "\n",
        "Sources: <https://learnpython.com/blog/python-set-operations/>\n"
      ],
      "id": "ccdd8fb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find active hospitals in 2016 (PGM_TRMNTN_CD==0)\n",
        "active_2016 = pos_df[(pos_df['Year'] == 2016) & (pos_df['PGM_TRMNTN_CD'] == 0)]['PRVDR_NUM'].unique()\n",
        "\n",
        "# Filter hospitals that have non-zero PGM_TRMNTN_CD in 2017, 2018, or 2019\n",
        "closed_hospitals = pos_df[(pos_df['PRVDR_NUM'].isin(active_2016)) & \n",
        "    (pos_df['Year'].isin([2017, 2018, 2019])) & \n",
        "    (pos_df['PGM_TRMNTN_CD'] != 0)]\n",
        "\n",
        "# Find hospitals present in data in 2016, but not in data after that\n",
        "present_after_2016 = pos_df[pos_df['Year'].isin([2017, 2018, 2019])]['PRVDR_NUM'].unique()\n",
        "potential_closures = set(active_2016) - set(present_after_2016)\n",
        "print(potential_closures) \n",
        "# empty, implying no hospitals are in the data that are only 2016\n",
        "\n",
        "# Find suspected year of closure\n",
        "year_closed = closed_hospitals.groupby('PRVDR_NUM')['Year'].min().reset_index() \n",
        "year_closed.columns = ['PRVDR_NUM', 'YR_CLOSED']\n",
        "\n",
        "suspected_closed = pos_df[pos_df['PRVDR_NUM'].isin(year_closed['PRVDR_NUM'])]\n",
        "# drop duplicate hospitals, by PRVDR_NUM\n",
        "suspected_closed = suspected_closed.drop_duplicates(subset=['PRVDR_NUM'])\n",
        "# Merge with suspected year of closure\n",
        "suspected_closed = suspected_closed.merge(year_closed, on=\"PRVDR_NUM\")\n",
        "# keep only desired columns \n",
        "suspected_closed = suspected_closed[['FAC_NAME', 'ZIP_CD', 'YR_CLOSED']]\n",
        "suspected_closed.head(3)\n",
        "\n",
        "print(f'There are {len(suspected_closed)} hospitals that fit this definition.')"
      ],
      "id": "7b00ce29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. "
      ],
      "id": "5c859534"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "suspected_closed[['FAC_NAME', 'YR_CLOSED']].sort_values(by=['FAC_NAME']).head(10)"
      ],
      "id": "83f68560",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.  However, not all suspected hospital closures are true closures. For example, in the case of a merger, a CMS certification number will be appear to be “terminated,” but then the hospital re-appear under a similar name/address with a new CMS certification number in the next year. As a first pass to address this, remove any suspected hospital closures that are in zip codes where the number of active hospitals does not decrease in the year after the suspected closure.\n",
        "\n",
        "Sources: <https://www.geeksforgeeks.org/apply-function-to-every-row-in-a-pandas-dataframe/>\n",
        "\n",
        "<https://discovery.cs.illinois.edu/guides/DataFrame-Fundamentals/run-a-function-on-rows-dataframe/>\n",
        "\n",
        "<https://flexiple.com/python/if-not-python>\n",
        "\n",
        "ChatGPT: I couldn't get the function to run row-wise properly. It suggested to alter my code to assign zip_code and year to their own variables. I also used it to figure out how to properly extract the number of hospitals in year_before and year_after (year_after['Hospital_Count'].values[0]). \n"
      ],
      "id": "42b46261"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "active_hospitals = pos_df[pos_df['PGM_TRMNTN_CD'] == 0]\n",
        "hospitals_per_year = active_hospitals.groupby(['ZIP_CD', 'Year']).size().reset_index(name='Hospital_Count')\n",
        "hospitals_per_year['Diff_Count'] = hospitals_per_year.groupby('ZIP_CD')['Hospital_Count'].diff()\n",
        "\n",
        "def check_decrease(row):\n",
        "    zip_code = row['ZIP_CD']\n",
        "    year = row['YR_CLOSED']\n",
        "    year_before = hospitals_per_year[(hospitals_per_year['ZIP_CD'] == zip_code) & (hospitals_per_year['Year'] == year)]\n",
        "    year_after = hospitals_per_year[(hospitals_per_year['ZIP_CD'] == zip_code) & (hospitals_per_year['Year'] == year + 1)]\n",
        "    \n",
        "    if not year_before.empty and not year_after.empty:\n",
        "        return year_after['Hospital_Count'].values[0] < year_before['Hospital_Count'].values[0]\n",
        "    return True\n",
        "\n",
        "# Apply check_decrease to suspected_closed and filter\n",
        "filtered_suspected_closed = suspected_closed[suspected_closed.apply(check_decrease, axis=1)]"
      ],
      "id": "b23df00a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    a."
      ],
      "id": "6f0b1b40"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "filtered_closed_names = filtered_suspected_closed['FAC_NAME']\n",
        "merger_acquisition = suspected_closed[~suspected_closed['FAC_NAME'].isin(filtered_closed_names)]\n",
        "\n",
        "print(f'Among the suspected closures, {len(merger_acquisition)} are suspected to be mergers and acquisitions.')\n",
        "```\n",
        "\n",
        "\n",
        "    b.\n",
        "\n",
        "```{python}\n",
        "print(f'After correcting for this, we have {len(filtered_suspected_closed)} left defined as closures.')\n",
        "```\n",
        "\n",
        "\n",
        "    c.\n",
        "\n",
        "```{python}\n",
        "filtered_suspected_closed.sort_values(by=['FAC_NAME']).head(10)\n",
        "```\n",
        "\n",
        "\n",
        "## Download Census zip code shapefile (10 pt) \n",
        "\n",
        "1. This is non-tabular data. \n",
        "    a. What are the five file types and what type of information is in each file?\n",
        "\n",
        "* Database file (dbf): attribute information for spatial features. Row = feature (point, line, etc.), column = specific attribute (name, etc.).\n",
        "* Projection file (prj): describes Coordinate Reference System (CRS) of the shapefile.\n",
        "* Shapefile (shp): feature geometrics data for spatial features, including coordinates. \n",
        "* Shape index file (shx): positional index\n",
        "* Extensible markup language (xml): metadata about spatial data\n",
        "\n",
        "    b. It will be useful going forward to have a sense going forward of which files are big versus small. After unzipping, how big is each of the datasets?\n",
        "\n",
        "\n",
        "```{python}\n",
        "import os\n",
        "\n",
        "file_types = ['.dbf', '.prj', '.shp', '.shx', '.xml']\n",
        "directory = '/Users/yunabaek/Desktop/3. Python II/gz_2010_us_860_00_500k' \n",
        "\n",
        "for file in os.listdir(directory):\n",
        "    if any(file.endswith(ext) for ext in file_types):\n",
        "        file_path = os.path.join(directory, file)\n",
        "        file_size = os.path.getsize(file_path)\n",
        "        print(f\"{file}: {file_size} bytes\")\n",
        "```\n",
        "\n",
        "\n",
        "2. Load the zip code shapefile and restrict to Texas zip codes. (Hint: you can identify which state a zip code is in using the first 2-3 numbers in the zip code. Then calculate the number of hospitals per zip code in 2016 based on the cleaned POS file from the previous step. Plot a choropleth of the number of hospitals by zip code in Texas.\n",
        "\n",
        "\n",
        "```{python}\n",
        "import geopandas as gpd\n",
        "# import fiona\n",
        "import pyogrio\n",
        "# shppath = r\"C:\\Users\\User\\OneDrive - The University of Chicago\\4_DAP-2\\gz_2010_us_860_00_500k\"\n",
        "shppath = '/Users/yunabaek/Desktop/3. Python II/problem-set-4-lizzy-yuna/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp'\n",
        "data_shp = gpd.read_file(shppath)\n",
        "texas_shp = data_shp[data_shp['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]\n",
        "```\n",
        "\n",
        "```{python}\n",
        "# Filtering shapefile\n",
        "# Converting into string\n",
        "hospitals_per_year['ZIP_CD'] = hospitals_per_year['ZIP_CD'].astype(str)\n",
        "\n",
        "# Subsetting for 2016\n",
        "hospitals_2016 = hospitals_per_year[\n",
        "    (hospitals_per_year['Year'] == 2016) &\n",
        "    (hospitals_per_year['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79')))\n",
        "]\n",
        "\n",
        "# Aggregating per zip-code count\n",
        "hospitals_count = hospitals_2016.groupby('ZIP_CD')['Hospital_Count'].sum().reset_index()\n",
        "hospitals_count.columns = ['ZCTA5', 'Number_of_Hospitals']\n",
        "```\n",
        "\n",
        "```{python}\n",
        "# Fixing variable types before merging\n",
        "hospitals_count['ZCTA5'] = hospitals_count['ZCTA5'].astype(str).str.replace('.0', '').astype(int)\n",
        "hospitals_count['ZCTA5'] = hospitals_count['ZCTA5'].astype(int)\n",
        "texas_shp['ZCTA5'] = texas_shp['ZCTA5'].astype(int)\n",
        "```\n",
        "\n",
        "\n",
        "Source: <https://geopandas.org/en/stable/docs/user_guide/mapping.html>\n",
        "\n",
        "```{python}\n",
        "# Merging the Texas shapefile with the hospital counts\n",
        "texas_hospitals = texas_shp.merge(hospitals_count, on='ZCTA5', how='left')\n",
        "\n",
        "# Fill NaN values with 0 (if any ZIP codes have no hospitals)\n",
        "texas_hospitals['Number_of_Hospitals'].fillna(0, inplace=True)\n",
        "\n",
        "texas_hospitals.plot(column='Number_of_Hospitals', legend=True,\n",
        "                     cmap='viridis',\n",
        "                     legend_kwds={'label': \"Number of Hospitals\",\n",
        "                                  'orientation': \"horizontal\"},\n",
        "                     vmin=0, vmax=5)\n",
        "\n",
        "plt.title('Number of Hospitals per ZIP Code in Texas (2016)')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "## Calculate zip code’s distance to the nearest hospital (20 pts) (*)\n",
        "\n",
        "1. Create a GeoDataFrame for the centroid of each zip code nationally: zips_all_centroids. What are the dimensions of the resulting GeoDataFrame and what do each of the columns mean?\n",
        "\n",
        "* ZCTA5: The zip code\n",
        "\n",
        "* geometry: The points representing the shape of each zip code. \n",
        "\n",
        "* centroid: The centroid point coordinates, which are an arithmetic mean position of all the points in a polygon.\n",
        "\n",
        "\n",
        "```{python}\n",
        "data_shp['centroid'] = data_shp['geometry'].centroid\n",
        "zips_all_centroids = data_shp[['ZCTA5', 'geometry','centroid']]\n",
        "zips_all_centroids.set_geometry('centroid')\n",
        "\n",
        "print(f'The dimensions of the dataframe are: {zips_all_centroids.shape}')\n",
        "```\n",
        "\n",
        "\n",
        "2. Create two GeoDataFrames as subsets of zips_all_centroids. First, create all zip codes in Texas: zips_texas_centroids. \n",
        "\n",
        "\n",
        "```{python}\n",
        "zips_texas_centroids = zips_all_centroids[\n",
        "    zips_all_centroids['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))\n",
        "]\n",
        "\n",
        "print(f'There are {zips_texas_centroids['ZCTA5'].nunique()} unique zip codes in this subset.')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Then, create all zip codes in Texas or a bordering state: zips_texas_borderstates_centroids.\n",
        "\n",
        "\n",
        "```{python}\n",
        "# Define zip code prefixes for Texas and neighboring states \n",
        "prefixes = ([str(x) for x in range (75, 79)]  + \n",
        "    [str(x) for x in range (716, 729)] + \n",
        "    [str(x) for x in range (700, 715)] + \n",
        "    [str(x) for x in range (870, 884)] + \n",
        "    [str(x) for x in range (73, 74)] \n",
        ")\n",
        "zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(tuple(prefixes))]\n",
        "\n",
        "print(f'There are {zips_texas_borderstates_centroids['ZCTA5'].nunique()} unique zip codes in this subset.')\n",
        "```\n",
        "\n",
        "\n",
        "3.  Then create a subset of zips_texas_borderstates_centroids that contains only the zip codes with at least 1 hospital in 2016. Call the resulting GeoDataFrame zips_withhospital_centroids What kind of merge did you decide to do, and what variable are you merging on?\n",
        "\n",
        "\n",
        "```{python}\n",
        "active_2016_df = pos_df[(pos_df['Year'] == 2016) & (pos_df['PGM_TRMNTN_CD'] == 0)]\n",
        "# Fix the zip code format\n",
        "active_2016_df['ZIP_CD'] = active_2016_df['ZIP_CD'].astype(str).str.split('.').str[0]\n",
        "\n",
        "zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(\n",
        "    active_2016_df[['ZIP_CD']], \n",
        "    left_on='ZCTA5', \n",
        "    right_on='ZIP_CD', \n",
        "    how='inner'\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "I used an inner merge, merging on the zip code variables in the two datasets. This will give only the rows with matching zip codes in both datasets. The active_2016_df contains information for all active hospitals, so if there is a zip code in that dataset which matches a zip code in the Texas and bordering states dataset, it will appear in zips_withhospital_centroids. \n",
        "\n",
        "4. For each zip code in zips_texas_centroids, calculate the distance to the nearest zip code with at least one hospital in zips_withhospital_centroids.\n",
        "\n",
        "    a.\n",
        "\n",
        "\n",
        "```{python}\n",
        "# Subset to 10 zip codes\n",
        "zips_texas_subset = zips_texas_centroids[:10]\n",
        "from timeit import default_timer as timer\n",
        "start = timer()\n",
        "nearest_hospital_subset = gpd.sjoin_nearest(\n",
        "    zips_texas_subset, \n",
        "    zips_withhospital_centroids, \n",
        "    how='inner', distance_col=\"distance\"\n",
        ")\n",
        "end = timer()\n",
        "time = (end - start)\n",
        "\n",
        "print(f'It took {time:.2f} seconds to run the subsetted merge.')\n",
        "estimate_mins = (time*(zips_texas_centroids.shape[0]/10))/60\n",
        "print(f'I estimate the entire procedure will take {estimate_mins:.2f} minutes.')\n",
        "```\n",
        "\n",
        "\n",
        "    b.\n",
        "\n",
        "\n",
        "```{python}\n",
        "start = timer()\n",
        "nearest_hospital = gpd.sjoin_nearest(\n",
        "    zips_texas_centroids, \n",
        "    zips_withhospital_centroids, \n",
        "    how='inner', distance_col=\"distance\"\n",
        ")\n",
        "end = timer()\n",
        "time = (end - start)\n",
        "print(f'It took {time/60:.2f} minutes to run the full merge.')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "    c. According to the .prj file, the units are in \"degree.\" According to [this site](https://www.sco.wisc.edu/2022/01/21/how-big-is-a-degree/), 1 degree is approximately equal to 69.4 miles. \n",
        "\n",
        "\n",
        "```{python}\n",
        "nearest_hospital['distance'] = nearest_hospital['distance']*69.4\n",
        "```\n",
        "\n",
        "\n",
        "5. Calculate the average distance to the nearest hospital for each zip code in Texas.\n",
        "\n",
        "\n",
        "```{python}\n",
        "average_distance = nearest_hospital.groupby(['ZCTA5_left'])['distance'].mean().reset_index()\n",
        "average_distance = average_distance.rename(columns={'ZCTA5_left':'zip'})\n",
        "average_distance.head(3)\n",
        "```\n",
        "\n",
        "\n",
        "    a. The units are in miles. \n",
        "\n",
        "    b.\n",
        "\n",
        "```{python}\n",
        "print(f'The average distance to the nearest hospital for zip codes in \\\n",
        "    Texas is {nearest_hospital.loc[:,\"distance\"].mean():.2f} miles.')\n",
        "```\n",
        "\n",
        "\n",
        "    c.\n",
        "\n",
        "```{python}\n",
        "# Merge TX zip shapes with avg distance to hospitals\n",
        "texas_hospitals_distance = zips_texas_centroids.merge(\n",
        "    average_distance, \n",
        "    left_on='ZCTA5', \n",
        "    right_on='zip', \n",
        "    how='outer'\n",
        ")\n",
        "\n",
        "texas_hospitals_distance.plot(column='distance', legend=True,\n",
        "    cmap='viridis',\n",
        "    legend_kwds={'label':'Miles',\n",
        "    'orientation':'horizontal'}\n",
        ")\n",
        "plt.title('Average Distance to Nearest Hospital by Zip Code')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "## Effects of closures on access in Texas (15 pts)\n",
        "\n",
        "1. Using the corrected hospital closures dataset from the first section, create a list of directly affected zip codes in Texas – that is, those with at least one closure in 2016-2019. Display a table of the number of zip codes vs. the number of closures they experienced.\n",
        "\n",
        "\n",
        "```{python}\n",
        "closure_df = pd.DataFrame(filtered_suspected_closed['ZIP_CD'].value_counts().reset_index())\n",
        "closure_df.columns = ['ZCTA5', 'Count']\n",
        "closure_df\n",
        "```\n",
        "\n",
        "\n",
        "2. Plot a choropleth of which Texas zip codes were directly affected by a closure in 2016-2019 – there was at least one closure within the zip code. How many directly affected zip codes are there in Texas?\n",
        "\n",
        "\n",
        "```{python}\n",
        "# Merging the Texas shapefile with the closure counts\n",
        "texas_closure_df = texas_shp.merge(closure_df, on='ZCTA5', how='left')\n",
        "\n",
        "# Fill NaN values with 0\n",
        "texas_closure_df['Count'].fillna(0, inplace=True)\n",
        "\n",
        "texas_closure_df.plot(column='Count', legend=True,\n",
        "                     cmap='viridis',\n",
        "                     legend_kwds={'label': \"Number of Affected Hospitals\",\n",
        "                                  'orientation': \"horizontal\"},\n",
        "                     vmin=0, vmax=5)\n",
        "\n",
        "plt.title('Number of Affected Hospitals by Zip Code')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "```{python}\n",
        "# Count of directly affected zip codes\n",
        "affected_zip_codes_count = len(texas_closure_df[texas_closure_df['Count'] > 0])\n",
        "print(f'There are {affected_zip_codes_count} zip codes in Texas that were directly affected.')\n",
        "```\n",
        "\n",
        "\n",
        "3. Then identify all the indirectly affected zip codes: Texas zip codes within a 10-mile radius of the directly affected zip codes. To do so, first create a GeoDataFrame of the directly affected zip codes. Then create a 10-mile buffer around them. Then, do a spatial join with the overall Texas zip code shapefile. How many indirectly affected zip codes are there in Texas?\n",
        "\n",
        "Attribution: <https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.set_geometry.html>, ChatGPT to resolve error on .set_geometry\n",
        "\n",
        "\n",
        "```{python}\n",
        "# Creating a category for affected directly / indirectly / not affected\n",
        "def affected(df):\n",
        "    df['affected'] = 'Not Affected'\n",
        "    df.loc[df['Count'] > 0, 'affected'] = 'Directly Affected'\n",
        "    return df\n",
        "\n",
        "texas_affected_df = affected(texas_closure_df)\n",
        "```\n",
        "\n",
        "```{python}\n",
        "# Creating GeoDataFrame\n",
        "affected_gdf = gpd.GeoDataFrame(\n",
        "    texas_affected_df[texas_affected_df['Count'] > 0], \n",
        "    geometry=texas_affected_df.geometry\n",
        ")\n",
        "# 10-mile (16093.4 meters) buffer\n",
        "affected_gdf['buffer'] = affected_gdf.geometry.buffer(16093.4)\n",
        "affected_gdf.set_geometry('buffer', inplace=True)\n",
        "```\n",
        "\n",
        "```{python}\n",
        "# Spatial join\n",
        "indirectly_affected_gdf = gpd.sjoin(texas_shp, affected_gdf, \n",
        "                                     how='inner', predicate='intersects')\n",
        "\n",
        "# Count the number of indirectly affected zip codes\n",
        "indirectly_affected_count = indirectly_affected_gdf['ZCTA5_left'].nunique()\n",
        "print(f'Number of indirectly affected zip codes in Texas: {indirectly_affected_count}')\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "4. Make a choropleth plot of the Texas zip codes with a different color for each of the 3 categories: directly affected by a closure, within 10 miles of closure but not directly affected, or not affected.\n",
        "\n",
        "\n",
        "```{python}\n",
        "# Adding indirectly affected category\n",
        "indirectly_affected_zip_codes = indirectly_affected_gdf['ZCTA5_left'].unique()\n",
        "\n",
        "def affected(df, indirectly_affected_zipcodes):\n",
        "    df['affected'] = 'Not Affected'\n",
        "    df.loc[df['Count'] > 0, 'affected'] = 'Directly Affected'\n",
        "    df.loc[df['ZCTA5'].isin(indirectly_affected_zipcodes), 'affected'] = 'Indirectly Affected'\n",
        "    return df\n",
        "\n",
        "texas_affected_df = affected(texas_closure_df, indirectly_affected_zip_codes)\n",
        "```\n",
        "\n",
        "\n",
        "Attribution: ChatGPT for fixing legend errors\n",
        "\n",
        "```{python}\n",
        "color = {\n",
        "    'Directly Affected': 'red',\n",
        "    'Indirectly Affected': 'purple',\n",
        "    'Not Affected': 'blue'\n",
        "}\n",
        "\n",
        "texas_affected_df['color'] = texas_affected_df['affected'].map(color)\n",
        "\n",
        "texas_affected_df.plot(color=texas_affected_df['color'], legend=False)\n",
        "plt.title('Number of Affected Hospitals by Zip Code')\n",
        "\n",
        "handles = [\n",
        "    plt.Line2D([0], [0], color='red', lw=4, label='Directly Affected'),\n",
        "    plt.Line2D([0], [0], color='purple', lw=4, label='Indirectly Affected'),\n",
        "    plt.Line2D([0], [0], color='blue', lw=4, label='Not Affected')\n",
        "]\n",
        "\n",
        "plt.legend(handles=handles, title=\"Affected Categories\", loc='upper right')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "## Reflecting on the exercise (10 pts) \n",
        "\n",
        "a. Some potential issues with identifying closures using this \"first pass\" method are: \n",
        "\n",
        "* Using the facility name versus the CMS code: A hospital could have changed names but not their code, or vice versa. This could cause issues in correctly identifying and matching hospitals across subsetted data. We should be clear on how codes and names are assigned and can change through time. A hospital name change could inaccurately be interpreted as a closure.\n",
        "\n",
        "* Location: A hospital might have closed in one zip code and re-opened nearby, but in another zip code. This would be reflected as a closure using our method, but in practice probably shouldn't be. \n",
        "\n",
        "* The data could be skewed based on when information is collected. A merger occuring in one year might not show up until the next year's data is collected, and would be inaccurately interpreted. \n",
        "\n",
        "We could do a better job at identifying hospital closures by ensuring that the CMS code does not change for a specific facility across time. If it does, we would want information on why. For example, if the code changed because the hospital switched ownership, we might incorrectly count a closure. We should cross-verify this information with other data sources. \n",
        "\n",
        "b. Consider the way we are identifying zip codes affected by closures. How well does this reflect changes in zip-code-level access to hospitals? Can you think of some ways to improve this measure?\n",
        "\n",
        "Challenges of this measure includes data accuracy and reliance on a single source of information to determine hospital closures. Hospitals may be identified as closed (affected) due to mergers, or there may be issues with identifying and recording temporary closures. Additionally, the 10-mile radius would have different significance in urban vs. rural areas. The type and size of hospitals as well as the range of the service that hospitals cover would also need to be considered. We may be able to improve this measure by tracking long-term trends to measure peramnent closures, or adopt a modified buffer measures to reflect the population of each region. "
      ],
      "id": "14e762a5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/yunabaek/Desktop/3. Python II/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}